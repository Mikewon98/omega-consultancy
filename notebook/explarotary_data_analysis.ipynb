{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fee40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb720e3e",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 1. DATA LOADING AND INITIAL EXPLORATION\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ef457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning process...\n",
      "==================================================\n",
      "\n",
      "Processing: ../data/Bank_of_Abyssinia_reviews.csv\n",
      "Original shape: (889, 5)\n",
      "Columns: ['review', 'rating', 'date', 'bank', 'source']\n",
      "Missing values:\n",
      "review    0\n",
      "rating    0\n",
      "date      0\n",
      "bank      0\n",
      "source    0\n",
      "dtype: int64\n",
      "Duplicates removed: 0\n",
      "Rows removed due to missing review text: 0\n",
      "Date format standardized to YYYY-MM-DD\n",
      "Final shape after cleaning: (889, 5)\n",
      "Date range: 2024-01-10 to 2025-06-05\n",
      "Rating distribution:\n",
      "rating\n",
      "1    412\n",
      "2     51\n",
      "3     56\n",
      "4     38\n",
      "5    332\n",
      "Name: count, dtype: int64\n",
      "Cleaned data saved to: ../data\\cleaned_Bank_of_Abyssinia_reviews.csv\n",
      "\n",
      "Processing: ../data/Commercial_Bank_of_Ethiopia_reviews.csv\n",
      "Original shape: (2180, 5)\n",
      "Columns: ['review', 'rating', 'date', 'bank', 'source']\n",
      "Missing values:\n",
      "review    0\n",
      "rating    0\n",
      "date      0\n",
      "bank      0\n",
      "source    0\n",
      "dtype: int64\n",
      "Duplicates removed: 0\n",
      "Rows removed due to missing review text: 0\n",
      "Date format standardized to YYYY-MM-DD\n",
      "Final shape after cleaning: (2180, 5)\n",
      "Date range: 2024-03-06 to 2025-06-08\n",
      "Rating distribution:\n",
      "rating\n",
      "1     387\n",
      "2     113\n",
      "3     127\n",
      "4     189\n",
      "5    1364\n",
      "Name: count, dtype: int64\n",
      "Cleaned data saved to: ../data\\cleaned_Commercial_Bank_of_Ethiopia_reviews.csv\n",
      "\n",
      "Processing: ../data/Dashen_Bank_reviews.csv\n",
      "Original shape: (409, 5)\n",
      "Columns: ['review', 'rating', 'date', 'bank', 'source']\n",
      "Missing values:\n",
      "review    0\n",
      "rating    0\n",
      "date      0\n",
      "bank      0\n",
      "source    0\n",
      "dtype: int64\n",
      "Duplicates removed: 0\n",
      "Rows removed due to missing review text: 0\n",
      "Date format standardized to YYYY-MM-DD\n",
      "Final shape after cleaning: (409, 5)\n",
      "Date range: 2025-01-11 to 2025-06-08\n",
      "Rating distribution:\n",
      "rating\n",
      "1     34\n",
      "2     18\n",
      "3     12\n",
      "4     22\n",
      "5    323\n",
      "Name: count, dtype: int64\n",
      "Cleaned data saved to: ../data\\cleaned_Dashen_Bank_reviews.csv\n",
      "\n",
      "==================================================\n",
      "COMBINED DATASET SUMMARY\n",
      "==================================================\n",
      "Total reviews: 3478\n",
      "Banks included: ['Bank of Abyssinia' 'Commercial Bank of Ethiopia' 'Dashen Bank']\n",
      "Date range: 2024-01-10 to 2025-06-08\n",
      "Rating distribution:\n",
      "rating\n",
      "1     833\n",
      "2     182\n",
      "3     195\n",
      "4     249\n",
      "5    2019\n",
      "Name: count, dtype: int64\n",
      "Source distribution:\n",
      "source\n",
      "Google Play    3478\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Combined clean dataset saved to: combined_clean_bank_reviews.csv\n",
      "\n",
      "Sample of cleaned data:\n",
      "                                              review  rating        date  \\\n",
      "0                                   it's not working       3  2025-06-05   \n",
      "1  Hello, I’m facing a problem with the BOA Mobil...       1  2025-06-03   \n",
      "2                                        exceptional       5  2025-06-03   \n",
      "3                               BoA Mobile good bank       5  2025-06-02   \n",
      "4                    this is worest app 24/7 loading       1  2025-06-01   \n",
      "\n",
      "        source               bank  \n",
      "0  Google Play  Bank of Abyssinia  \n",
      "1  Google Play  Bank of Abyssinia  \n",
      "2  Google Play  Bank of Abyssinia  \n",
      "3  Google Play  Bank of Abyssinia  \n",
      "4  Google Play  Bank of Abyssinia  \n",
      "\n",
      "==================================================\n",
      "DATA QUALITY REPORT\n",
      "==================================================\n",
      "Dataset shape: (3478, 5)\n",
      "Memory usage: 1157.64 KB\n",
      "\n",
      "Column data types:\n",
      "review    object\n",
      "rating     int64\n",
      "date      object\n",
      "source    object\n",
      "bank      object\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "review    0\n",
      "rating    0\n",
      "date      0\n",
      "source    0\n",
      "bank      0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Unique values per column:\n",
      "  review: 3364\n",
      "  rating: 5\n",
      "  date: 502\n",
      "  source: 1\n",
      "  bank: 3\n",
      "\n",
      "Review text statistics:\n",
      "  Average length: 59.3 characters\n",
      "  Min length: 1\n",
      "  Max length: 500\n",
      "  Reviews with length < 10: 660\n",
      "\n",
      "Rating statistics:\n",
      "count    3478.000000\n",
      "mean        3.701265\n",
      "std         1.706761\n",
      "min         1.000000\n",
      "25%         2.000000\n",
      "50%         5.000000\n",
      "75%         5.000000\n",
      "max         5.000000\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "✅ Data cleaning completed successfully!\n",
      "Ready for sentiment and thematic analysis.\n"
     ]
    }
   ],
   "source": [
    "def clean_bank_reviews():\n",
    "    file_paths = [\n",
    "    '../data/Bank_of_Abyssinia_reviews.csv',\n",
    "    '../data/Commercial_Bank_of_Ethiopia_reviews.csv', \n",
    "    '../data/Dashen_Bank_reviews.csv'\n",
    "    ]\n",
    "    \n",
    "    # List to store cleaned dataframes\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    print(\"Starting data cleaning process...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\nProcessing: {file_path}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Original shape: {df.shape}\")\n",
    "            \n",
    "            # Display basic info about the dataset\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "            \n",
    "            # 1. Remove duplicates\n",
    "            initial_rows = len(df)\n",
    "            df = df.drop_duplicates()\n",
    "            duplicates_removed = initial_rows - len(df)\n",
    "            print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "            \n",
    "            # 2. Handle missing data\n",
    "            # Critical columns: review text cannot be missing\n",
    "            df_before_critical = len(df)\n",
    "            df = df.dropna(subset=['review'])\n",
    "            critical_rows_removed = df_before_critical - len(df)\n",
    "            print(f\"Rows removed due to missing review text: {critical_rows_removed}\")\n",
    "            \n",
    "            # Non-critical columns: fill missing values appropriately\n",
    "            # Fill missing ratings with median rating\n",
    "            if df['rating'].isnull().any():\n",
    "                median_rating = df['rating'].median()\n",
    "                df['rating'].fillna(median_rating, inplace=True)\n",
    "                print(f\"Missing ratings filled with median: {median_rating}\")\n",
    "            \n",
    "            # Fill missing dates with a placeholder or current date\n",
    "            if df['date'].isnull().any():\n",
    "                df['date'].fillna('2025-01-01', inplace=True)\n",
    "                print(\"Missing dates filled with placeholder date\")\n",
    "            \n",
    "            # Fill missing source with 'Unknown'\n",
    "            if 'source' in df.columns and df['source'].isnull().any():\n",
    "                df['source'].fillna('Unknown', inplace=True)\n",
    "                print(\"Missing sources filled with 'Unknown'\")\n",
    "            \n",
    "            # 3. Standardize date format using strftime\n",
    "            try:\n",
    "                # Convert to datetime first\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                # Format to YYYY-MM-DD using strftime\n",
    "                df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "                print(\"Date format standardized to YYYY-MM-DD\")\n",
    "            except Exception as e:\n",
    "                print(f\"Date formatting error: {e}\")\n",
    "            \n",
    "            # 4. Clean and standardize text data\n",
    "            # Remove extra whitespace from review text\n",
    "            df['review'] = df['review'].astype(str).str.strip()\n",
    "            \n",
    "            # Remove any empty string reviews that might have passed the null check\n",
    "            df = df[df['review'] != '']\n",
    "            \n",
    "            # 5. Ensure rating is numeric and within valid range (1-5)\n",
    "            df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "            df = df[(df['rating'] >= 1) & (df['rating'] <= 5)]\n",
    "            \n",
    "            # 6. Select only the required columns in the correct order\n",
    "            required_columns = ['review', 'rating', 'date', 'source']\n",
    "            \n",
    "            # Add bank column if it exists, otherwise create from filename\n",
    "            if 'bank' not in df.columns:\n",
    "                bank_name = file_path.replace('_reviews.csv', '').replace('_', ' ')\n",
    "                df['bank'] = bank_name\n",
    "            \n",
    "            # Reorder columns (including bank for reference, can be removed later if needed)\n",
    "            final_columns = ['review', 'rating', 'date', 'source', 'bank']\n",
    "            df = df[final_columns]\n",
    "            \n",
    "            print(f\"Final shape after cleaning: {df.shape}\")\n",
    "            print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "            print(f\"Rating distribution:\\n{df['rating'].value_counts().sort_index()}\")\n",
    "            \n",
    "            # Save cleaned dataset\n",
    "            # Extract filename only\n",
    "            file_dir = os.path.dirname(file_path)  # ../data\n",
    "            file_name = os.path.basename(file_path)  # Bank_of_Abyssinia_reviews.csv\n",
    "\n",
    "            # Construct new file path in the same directory\n",
    "            output_filename = os.path.join(file_dir, f\"cleaned_{file_name}\") \n",
    "\n",
    "            os.makedirs(file_dir, exist_ok=True)\n",
    "\n",
    "            # Save cleaned file\n",
    "            df.to_csv(output_filename, index=False)\n",
    "            print(f\"Cleaned data saved to: {output_filename}\")\n",
    "            cleaned_dfs.append(df)\n",
    "            \n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Combine all cleaned datasets\n",
    "    if cleaned_dfs:\n",
    "        combined_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"COMBINED DATASET SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total reviews: {len(combined_df)}\")\n",
    "        print(f\"Banks included: {combined_df['bank'].unique()}\")\n",
    "        print(f\"Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "        print(f\"Rating distribution:\\n{combined_df['rating'].value_counts().sort_index()}\")\n",
    "        print(f\"Source distribution:\\n{combined_df['source'].value_counts()}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        combined_df.to_csv('../data/.combined_clean_bank_reviews.csv', index=False)\n",
    "        print(f\"\\nCombined clean dataset saved to: combined_clean_bank_reviews.csv\")\n",
    "        \n",
    "        # Display sample of cleaned data\n",
    "        print(f\"\\nSample of cleaned data:\")\n",
    "        print(combined_df.head())\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    else:\n",
    "        print(\"No datasets were successfully processed.\")\n",
    "        return None\n",
    "\n",
    "# Data quality check function\n",
    "def data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "    \n",
    "    print(f\"\\nColumn data types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    print(f\"\\nUnique values per column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}: {df[col].nunique()}\")\n",
    "    \n",
    "    print(f\"\\nReview text statistics:\")\n",
    "    review_lengths = df['review'].str.len()\n",
    "    print(f\"  Average length: {review_lengths.mean():.1f} characters\")\n",
    "    print(f\"  Min length: {review_lengths.min()}\")\n",
    "    print(f\"  Max length: {review_lengths.max()}\")\n",
    "    print(f\"  Reviews with length < 10: {(review_lengths < 10).sum()}\")\n",
    "    \n",
    "    print(f\"\\nRating statistics:\")\n",
    "    print(df['rating'].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clean the datasets\n",
    "    combined_data = clean_bank_reviews()\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        # Generate quality report\n",
    "        data_quality_report(combined_data)\n",
    "        \n",
    "        print(f\"\\n✅ Data cleaning completed successfully!\")\n",
    "        print(f\"Ready for sentiment and thematic analysis.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Data cleaning failed. Please check file paths and data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
